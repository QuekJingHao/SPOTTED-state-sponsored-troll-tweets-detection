{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bd5b084-ffef-4d76-9cab-9a2cc0520859",
   "metadata": {},
   "source": [
    "# **Project SPOTTED: - Data Selection, Collection and Sampling**\n",
    "\n",
    "**<u>_Objective:_</u>** We fine-tune a pretrained BERTModel, to predict if a tweet is made by an information operative (state-sponsored troll) or not. The aim is to increase the efficiency of \n",
    "\n",
    "\n",
    "from state trolls for the defense and intelligence community.\n",
    "\n",
    "---\n",
    "In this notebook, I describe the approach in the selection, collection and sampling of data that will be used in Project SPOTTED.\n",
    "\n",
    "Data forms the bedrock of any machine learning models. During the collection phase, we must ensure the data integrity is maintained, so that the data is accurate and consistent over its life cycle. At worst, if the data is biased, or is unrepresentative from the get go, then all our efforts will be in vain when the biased data is feed into our models. Before we begin, we recognize the limitations in dealing with very large datasets in Pandas. When unzipped, the size of the entire troll dataset archive is **118 Gb**. Furthermore, Pandas is prone to crashing when it reads csv whose size is larger than 3 gb. To go around this, we shall use random sampling to help us obtain a sample that is as representative as possible to the entire population dataset. \n",
    "\n",
    "We aim to produce a dataset of **200 000 tweets** for Project SPOTTED. First, I will explain my approach in selecting the datasource as the first step of the collection process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edfc4b0-705d-4e7e-8da9-ae20b3001d58",
   "metadata": {},
   "source": [
    "### Data Selection\n",
    "\n",
    "The procedure to produce the 200 000 tweets can be summarized as follows:\n",
    "1. Choose 20 troll datasets from the Twitter Moderation Research Consortium (see below), randomly sample 10 000 tweets from each where possible\n",
    "2. Choose 50 verified Twitter accounts and perform stratified random sampling:\n",
    "   - Categorize the 50 accounts into 5 strata\n",
    "   - For each account within each stratum, collect 2500 to 3500 tweets where possible\n",
    "   - Randomly sample tweets within each strata accordingly to the strata's significance (given in Table 2)\n",
    "   - Pool all of the random samples together - giving a total of 100 000 tweets - to form the verified Twitter dataset. Randomly shuffle the dataset.\n",
    "3. Concatenate these two large datasets. Randomly shuffle the dataset\n",
    "4. Split the first 150 000 rows as **Training Dataset**. This will be the dataset will enter into the train, test  split function for training the ML model.\n",
    "5. Split the last 5000 rows as **Validation Dataset**. This will be the dataset unseen by the model, which will be used by the model to predict if the tweet is made by a troll or not.\n",
    "\n",
    "The overview of the entire collection process can be summarized in the following chart: \n",
    "\n",
    "\n",
    "<p align=\"center\">  \n",
    "  <img src=\"https://github.com/QuekJingHao/google-data-analytics-capstone-project/blob/main/4 Images/db_overall.png\" width=\"800\" height=\"600\">\n",
    "</p>\n",
    "\n",
    "\n",
    "In selecting our data, we aim to achieve the following objectives:\n",
    "1. Sample from the entire repository between 2018 and 2021 \n",
    "   - The reason is simple: state actors change and modify their MO and tradecraft over time. For example, Russian trolls may target the 2016 elections by spreading disinformation. However in 2021, they may attack the botched American withdrawal from Afghanistan, by amplifying the number of American casualties.\n",
    "2. Select multiple verified Twitter accounts\n",
    "   - We again should be as representative as possible to cover a large range of topics.\n",
    "3. Ensure there is no data leakage of validation data into training dataset\n",
    "   - We should remove any duplicated tweets if present. We should also randomly shuffle the dataset before we split it into training or validation datasets. In this way, we ensure that the none of the tweets used for validation somehow gets into the training dataset. If this happens, then the performance of the model will be much better than expected.\n",
    "   <br></br>\n",
    "\n",
    "In the next two subsections, I'll describe the selection process for the troll and clean datasets. \n",
    "<br></br>\n",
    "\n",
    "\n",
    "\n",
    "#### <u>Selection of State-linked Troll Datasets</u>\n",
    "\n",
    "The information ops dataset can be downloaded from the Twitter Moderation Research Consortium (TMRC) via the following link: https://transparency.twitter.com/en/reports/moderation-research.html. The TMRC overseas moderation and transparency on Twitter. Such measures include the disclosure of state-linked operatives engaging in platform manipulation and information operations. With the formation of the Consortium, newer datasets are no longer public but shared with members of the Consortium. Nonetheless, the goldmine of an archive released from 2018 to 2021 is more than sufficient for our purpose. \n",
    "\n",
    "The following table lists all of the 20 selected troll datasets:\n",
    "\n",
    "<center>\n",
    "\n",
    "|                                        |                                                |\n",
    "|:--------------------------------------:|:----------------------------------------------:|\n",
    "| 1.  IRA Oct 2018                       | 11.  Indonesia Feb 2020                        |\n",
    "| 2.  Iran Oct 2018                      | 12.  China May 2020                            |\n",
    "| 3.  Russia Jan 2019                    | 13.  Russia May 2020 $\\dagger$                 |\n",
    "| 4.  Venezuela Jun 2019 $\\dagger$       | 14.  Thailand Sept 2020                        |\n",
    "| 5.  Iran Jan 2019 $\\dagger$            | 15.  Iran Feb 2020                             |\n",
    "| 6.  Iran (Set 1) June 2019             | 16.  Russia GRU Feb 2021                       |\n",
    "| 7.  Iran (Set 2) June 2019             | 17.  Russia IRA Feb 2014                       |\n",
    "| 8.  China (Set 1) Aug 2019             | 18.  China Changyu Culture Dec 2021 $\\dagger$  |\n",
    "| 9.  China (Set 2) Aug 2019             | 19.  China Xinjiang Dec 2021 $\\dagger$         |\n",
    "| 10. China (Set 3) Sept 2019 $\\dagger$  | 20.  Venezuela Dec 2021 $\\dagger$              |\n",
    "\n",
    "</center>\n",
    "\n",
    "$\\dagger$ contains multiple CSVs to be merged\n",
    "\n",
    "\n",
    "We will pick only the English tweets, drop any duplicates and mine 10 000 tweets from each of these datasets.\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a8488c",
   "metadata": {},
   "source": [
    "\n",
    "#### <u>Selection of Verified Twitter Users</u>\n",
    "\n",
    "In selecting the accounts that make up the clean dataset, we have to pay close attention to the troll dataset. An exploratory data analysis (another notebook) on the troll datasets reveals some key aspects of the troll's MO:\n",
    "1. The earliest information operations dates back to 2009\n",
    "2. Majority of the trolls target US government, society and politics\n",
    "3. Majority disguise their tweets as genuine \"news\", or amplify events detrimental to US interests and national security\n",
    "4. Many disguise as genuine persons using Twitter, albeit parroting extreme and diversive political / social views and amplifying existing grieviances\n",
    "\n",
    "Hence, we in selecting the verified accounts, we aim to mirror the above mentioned dynamic: we shall collect from many verified news media accounts, and include a fraction of US government accounts. However, we should avoid restricting the clean dataset to just focus on the US or international news. So I have thrown in several accounts related to Singapore. Lastly, some accounts related to entertainment, science and technology is included for good measure.  \n",
    "\n",
    "Below shows the verified Twitter accounts, categorized into 5 different strata:\n",
    "\n",
    "<center>\n",
    "\n",
    "|             |                                      |                             |                                  |                               |                          |\n",
    "|:-----------:|:-------------------------------------|:----------------------------|:---------------------------------|:------------------------------|:--------------------------|\n",
    "|**[Strata]**   | US Politics                          | US Military                 | Singapore Government             | Entertainment, Science & Tech | International News        |\n",
    "|**[Accounts]** | President of the United States       | Department of Defense       | PAP                              | Guns n Roses                  | CNN                       |\n",
    "|             | Vice President of the United States  | US Cyber Command            | WP                               | Metallica                     | BBC World                 |\n",
    "|             | The White House                      | Defense Intelligence Agency | Ministry of Defense              | NASA                          | New York Times            |\n",
    "|             | Hillary Clinton                      | Central Intelligence Agency | Republic of Singapore Air Force  | Google                        | Washington Post           |\n",
    "|             | Barack Obama                         | US Army                     | Gov Singapore                    | SpaceX                        | The Straits Times         |\n",
    "|             |                                      | US Air Force                | Ministry of Home Affairs         |                               | Channel News Asia         |\n",
    "|             |                                      | US Navy                     | Ministry of Education            |                               | TODAY Online              |\n",
    "|             |                                      | US Marine Corps             | Ministry of Foreign Affairs      |                               | Washington Street Journal |\n",
    "|             |                                      | Indo-Pacific Command        | Ministry of Health               |                               | Reuters                   |\n",
    "|             |                                      |                             |                                  |                               | The Economist             |\n",
    "|             |                                      |                             |                                  |                               | Financial Times           |\n",
    "|             |                                      |                             |                                  |                               | Bloomberg                 |\n",
    "|             |                                      |                             |                                  |                               | Forbes                    |\n",
    "|             |                                      |                             |                                  |                               | CNBC                      |\n",
    "|             |                                      |                             |                                  |                               | MSNBC                     |\n",
    "|             |                                      |                             |                                  |                               | CBS News                  |\n",
    "|             |                                      |                             |                                  |                               | ABC                       |\n",
    "|             |                                      |                             |                                  |                               | CNN International         |\n",
    "|             |                                      |                             |                                  |                               | New York Times World      |\n",
    "\n",
    "</center>\n",
    "\n",
    "The collection of Tweets is done using the OSINT package - Twitter Intelligence Tool (TWINT) - which bypasses the need for Twitter API.\n",
    "\n",
    "The number of tweets we will sample from each of these strata is given in the following table:\n",
    "\n",
    "<center>\n",
    "\n",
    "| Strata                          | Number of tweets to randomly sample    |\n",
    "|:-------------------------------:|:--------------------------------------:|\n",
    "| US Politics                     | 10 000                                 |\n",
    "| US Military                     | 10 000                                 |\n",
    "| Singapore Government            | 10 000                                 |\n",
    "| Entertainment, Science & Tech   | 10 000                                 |\n",
    "| International News              | 70 000                                 |\n",
    "\n",
    "Table 2: Number of tweets to randomly sample within each strata\n",
    "    \n",
    "</center>\n",
    "\n",
    "The stratified random sampling process can be summarized in the following chart:\n",
    "\n",
    "<p align=\"center\">  \n",
    "  <img src=\"https://github.com/QuekJingHao/google-data-analytics-capstone-project/blob/main/4 Images/db_overall.png\" width=\"800\" height=\"600\">\n",
    "</p>\n",
    "\n",
    "Lastly, we encode the troll and verified datasets as follows:\n",
    "\n",
    "<center>\n",
    "\n",
    "|            |      |  \n",
    "|:----------:|:----:|\n",
    "| Troll:     | 1    |\n",
    "| Verified:  | 0    |\n",
    "\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d07e2c-a1b7-49a6-8a11-89c96c77d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules and dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import twint\n",
    "import os\n",
    "import random\n",
    "import shutil as sh\n",
    "import nest_asyncio\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "from Data_Collection_Utility import *\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec54b214-c9f7-4e89-8623-566ee07e42b6",
   "metadata": {},
   "source": [
    "### Sampling Troll Dataset\n",
    "\n",
    "We would need to combine the datasets with the dagger first. Then afterwhich we sample 10 000 from each of the datasets, and combine them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1120652d-ca5a-42cf-82ae-aa7064deb370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different paths for running the notebook on thinkpad\n",
    "troll_path = 'E:/SPOTTED Data Collection/Data/Troll/'\n",
    "#troll_path = 'C:/Users/jh.quek/Documents/SPOTTED Data Collection/Data/Troll/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00706233-36d8-4aec-8ebb-62cd9d1018cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining Venezuela_Jan_2019\n",
      "['Venezuela_Jan_2019_P1.csv', 'Venezuela_Jan_2019_P2.csv', 'Venezuela_Jan_2019_P3.csv', 'Venezuela_Jan_2019.csv']\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Combining Iran_Jan_2019\n",
      "['Iran_Jan_2019_P1.csv', 'Iran_Jan_2019_P2.csv', 'Iran_Jan_2019_P3.csv', 'Iran_Jan_2019_P4.csv', 'Iran_Jan_2019.csv']\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Combining China_S3_Sept_2019\n",
      "['China_S3_Aug_2019_P1.csv', 'China_S3_Aug_2019_P2.csv', 'China_S3_Aug_2019_P3.csv', 'China_S3_Sept_2019.csv']\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Combining Russia_May_2020\n",
      "['Russia_May_2020_P1.csv', 'Russia_May_2020_P2.csv', 'Russia_May_2020.csv']\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Combining China_Changyu_Culture_Dec_2021\n",
      "['China_Changyu_Culture_hashed_2012.csv', 'China_Changyu_Culture_hashed_2013.csv', 'China_Changyu_Culture_hashed_2014.csv', 'China_Changyu_Culture_hashed_2015.csv', 'China_Changyu_Culture_hashed_2016.csv', 'China_Changyu_Culture_hashed_2017.csv', 'China_Changyu_Culture_hashed_2018.csv', 'China_Changyu_Culture_hashed_2019.csv', 'China_Changyu_Culture_hashed_2020.csv', 'China_Changyu_Culture_hashed_2021.csv', 'China_Changyu_Culture_Dec_2021.csv']\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Combining China_Xinjiang_Dec_2021\n",
      "['China_Xinjiang_hashed_2019.csv', 'China_Xinjiang_hashed_2020.csv', 'China_Xinjiang_hashed_2021.csv', 'China_Xinjiang_Dec_2021.csv']\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Combining Venezuela_Dec_2021\n",
      "['Venezuela_hashed_2009.csv', 'Venezuela_hashed_2010.csv', 'Venezuela_hashed_2011.csv', 'Venezuela_hashed_2012.csv', 'Venezuela_hashed_2013.csv', 'Venezuela_hashed_2014.csv', 'Venezuela_hashed_2015.csv', 'Venezuela_hashed_2016.csv', 'Venezuela_hashed_2017.csv', 'Venezuela_hashed_2018.csv', 'Venezuela_hashed_2019.csv', 'Venezuela_hashed_2020.csv', 'Venezuela_hashed_2021.csv', 'Venezuela_Dec_2021.csv']\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multiple_datasets = ['Venezuela_Jan_2019', 'Iran_Jan_2019', 'China_S3_Sept_2019', 'Russia_May_2020', \n",
    "                     'China_Changyu_Culture_Dec_2021', 'China_Xinjiang_Dec_2021', 'Venezuela_Dec_2021']\n",
    "\n",
    "for dataset in multiple_datasets:\n",
    "    print('Combining', dataset)\n",
    "    list_of_datasets = os.listdir(troll_path + dataset)\n",
    "    \n",
    "    print(list_of_datasets)\n",
    "    \n",
    "    combined_df = dataset_fusion(troll_path + dataset + '/', list_of_datasets)\n",
    "    \n",
    "    # Now that we are executing everything inside the hard drive we just save the merged dataframe inside\n",
    "    combined_df.to_csv(troll_path + dataset + '/' + dataset + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20791a9-3bf6-43dc-a697-59badfe1dbb6",
   "metadata": {},
   "source": [
    "Now, we are ready to sample 10 000 tweets from each of the 20 datasets and save them into a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2454d4-373f-428a-a8ee-56d4cdbfdc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of China_Changyu_Culture_Dec_2021_Sample.csv dataframe is 4777\n",
      "Length of China_May_2020_Sample.csv dataframe is 10000\n",
      "Length of China_S1_Aug_2019_Sample.csv dataframe is 10000\n",
      "Length of China_S2_Aug_2019_Sample.csv dataframe is 10000\n",
      "Length of China_S3_Sept_2019_Sample.csv dataframe is 10000\n",
      "Length of China_Xinjiang_Dec_2021_Sample.csv dataframe is 10000\n",
      "Length of Indonesia_Feb_2020_Sample.csv dataframe is 10000\n",
      "Length of Iran_Feb_2021_Sample.csv dataframe is 10000\n",
      "Length of Iran_Jan_2019_Sample.csv dataframe is 10000\n",
      "Length of Iran_Oct_2018_Sample.csv dataframe is 10000\n",
      "Length of Iran_S1_June_2019_Sample.csv dataframe is 10000\n",
      "Length of Iran_S2_June_2019_Sample.csv dataframe is 10000\n",
      "Length of Russia_GRU_Feb_2021_Sample.csv dataframe is 10000\n",
      "Length of Russia_IRA_Feb_2021_Sample.csv dataframe is 10000\n",
      "Length of Russia_IRA_Oct_2018_Sample.csv dataframe is 10000\n",
      "Length of Russia_Jan_2019_Sample.csv dataframe is 10000\n",
      "Length of Russia_May_2020_Sample.csv dataframe is 10000\n",
      "Length of Thailand_Sept_2020_Sample.csv dataframe is 162\n",
      "Length of Venezuela_Dec_2021_Sample.csv dataframe is 8981\n",
      "Length of Venezuela_Jan_2019_Sample.csv dataframe is 10000\n",
      "Length of merged dataframe is 183920, [True]\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Length of combined sampled troll dataset is 183920\n"
     ]
    }
   ],
   "source": [
    "# Need some clever methods to sample the huge datasets if executing on Acer\n",
    "troll_datasets = [directory for directory in os.listdir(troll_path) if directory != 'Troll_Samples']\n",
    "big_datasets = ['China_S3_Sept_2019', 'Iran_Jan_2019', 'Russia_IRA_Oct_2018', \n",
    "                'Russia_May_2020', 'Venezuela_Jan_2019']\n",
    "\"\"\"\n",
    "# Sampling 10000 rows from each of the 20 troll datasets - with a random state of 5 for reproductivity\n",
    "i = 0\n",
    "nrows = 10000\n",
    "for dataset in troll_datasets:\n",
    "    \n",
    "    # for the very big datasets; we'll make pandas randomly read 80% of the entire dataset\n",
    "    if dataset in big_datasets:\n",
    "        p = 0.8\n",
    "        np.random.seed(3)\n",
    "        df = pd.read_csv('D:/SPOTTED Data Collection/Data/Troll/{}/{}.csv'.format(dataset, dataset), \n",
    "                         skiprows = lambda x : x > 0 and np.random.rand(1)[0] > p,\n",
    "                         low_memory = False)\n",
    "    else:\n",
    "        df = pd.read_csv('D:/SPOTTED Data Collection/Data/Troll/{}/{}.csv'.format(dataset, dataset), \n",
    "                         low_memory = False)\n",
    "\n",
    "    # we pick only the English tweets, \n",
    "    df_sample = df[df['tweet_language'] == 'en']\n",
    "    \n",
    "    # this step may blow up if the number of rows of english tweets is already less than the specified number\n",
    "    try:\n",
    "        df_sample = df_sample.sample(n = nrows, random_state = 5)\n",
    "    except:\n",
    "        df_sample = df_sample.sample(frac = 1.0, random_state = 5)\n",
    "    \n",
    "    df_sample.to_csv('D:/SPOTTED Data Collection/Data/Troll/Troll_Samples/{}_Sample.csv'.format(dataset, dataset))\n",
    "    \n",
    "    print('[*]-- Point [{}] --------- Sampling {} Complete --------- Length of dataframe: [{}] [*]'.format(i + 1, dataset, len(df_sample)))\n",
    "    i += 1\n",
    "\"\"\"\n",
    "    \n",
    "# lastly, concatentate the 20 samples into one sample and write to csv\n",
    "troll_sample_datasets = [name + '_Sample.csv' for name in troll_datasets]\n",
    "troll_combined_df = dataset_fusion(troll_path + 'Troll_Samples/', troll_sample_datasets)\n",
    "print('Length of combined sampled troll dataset is', len(troll_combined_df))\n",
    "\n",
    "# randomly shuffle the dataframe with random state of 10\n",
    "troll_combined_df  = troll_combined_df.sample(frac = 1.0, random_state = 10)\n",
    "troll_combined_df.to_csv(troll_path + 'Troll_Dataset_Combined.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d545e5-0407-4f83-b434-bddcbdf415ec",
   "metadata": {},
   "source": [
    "### Downloading Clean Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f42402-d56e-4e37-942f-c886e0d3a4bd",
   "metadata": {},
   "source": [
    "Parameters specifications - we specify the limit of tweets to be extracted from each of the 50 accounts.\n",
    "\n",
    "We also subject the scrapper to the following constraints:\n",
    "* The date of the latest tweet should be on 1 Janurary 2022.\n",
    "  - This is because of the latest troll tweet is around that time. We have to match the period of the clean data with that of the troll dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c274761-c2d5-4caf-8449-71b876dd97df",
   "metadata": {},
   "outputs": [],
   "source": [
    "verified_path = 'E:/SPOTTED Data Collection/Data/Verified/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c023e36c-b48b-4212-96b7-6217ca3b8ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of accounts in each of these categories:\n",
      "      US Politics: 5\n",
      "      US Military: 10\n",
      "      SG Government: 10\n",
      "      EST: 5\n",
      "      International News: 20\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "# below is the 5 strata we will be working with. Each element in the arrays is the username of the accounts\n",
    "US_Politics_Stratum = ['POTUS', 'VP', 'WhiteHouse', 'BarackObama', 'HillaryClinton'] \n",
    "\n",
    "US_Military_Stratum = ['DeptofDefense', 'US_CYBERCOM', 'DefenseIntel', 'NSAGov', 'CIA', \n",
    "                       'USArmy', 'usairforce',  'USNavy', 'USMC', 'INDOPACOM']\n",
    "\n",
    "SG_Stratum = ['PAPSingapore', 'wpsg', 'mindefsg', 'SingaporePolice', 'TheRSAF',\n",
    "              'govsingapore', 'mhasingapore', 'MOEsg', 'MFAsg', 'sporeMOH']\n",
    "\n",
    "EST_Stratum = ['gunsnroses', 'Metallica', 'NASA', 'SpaceX', 'Google']\n",
    "    \n",
    "International_News_Stratum = ['CNN', 'BBCWorld', 'nytimes', 'TIME', 'washingtonpost',\n",
    "                              'straits_times', 'ChannelNewsAsia', 'TODAYonline', 'WSJ', 'Reuters',\n",
    "                              'TheEconomist', 'FT', 'business', 'Forbes', 'CNBC',\n",
    "                              'MSNBC', 'CBSNews', 'ABC', 'cnni', 'nytimesworld']\n",
    "\n",
    "print('Number of accounts in each of these categories:\\n\\\n",
    "      US Politics: {}\\n\\\n",
    "      US Military: {}\\n\\\n",
    "      SG Government: {}\\n\\\n",
    "      EST: {}\\n\\\n",
    "      International News: {}\\n\\\n",
    "      '.format(len(US_Politics_Stratum), len(US_Military_Stratum), \n",
    "               len(SG_Stratum), len(EST_Stratum), len(International_News_Stratum)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c04366-455d-4b21-a987-18345b07153a",
   "metadata": {},
   "source": [
    "Now we will mine the verified accounts above for the 2000 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986bb45b-9bcd-42ba-ba7a-36477320a339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Tweets on POTUS ...\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Length of dataframe: 2056\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on VP ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on WhiteHouse ...\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Length of dataframe: 2283\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on BarackObama ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on HillaryClinton ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "\n",
      "[*] Collection Successful. Total number of scrapped users: 5\n",
      "Moving files to designated folder...\n",
      "[*]--------------------------------------      COMPLETE      --------------------------------------[*]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mine US politicans category\n",
    "Twint_Scrapper(2500, US_Politics_Stratum, True, True, True)\n",
    "move_verified_datasets([file + '.csv' for file in US_Politics_Stratum], 'US_Politics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da92c7e9-2d9f-4edf-8c4f-61d5646f92f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Tweets on DeptofDefense ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on US_CYBERCOM ...\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Length of dataframe: 334\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on DefenseIntel ...\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Length of dataframe: 2376\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on NSAGov ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on CIA ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on USArmy ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on usairforce ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on USNavy ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on USMC ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on INDOPACOM ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "\n",
      "[*] Collection Successful. Total number of scrapped users: 10\n",
      "Moving files to designated folder...\n",
      "[*]--------------------------------------      COMPLETE      --------------------------------------[*]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mine US military category\n",
    "Twint_Scrapper(2500, US_Military_Stratum, True, True, True)\n",
    "move_verified_datasets([file + '.csv' for file in US_Military_Stratum], 'US_Military')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9bd59b-7a13-4eaf-a4df-b0d60f1a504c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Tweets on PAPSingapore ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on wpsg ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on mindefsg ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on SingaporePolice ...\n",
      "Length of dataframe: 2513\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on TheRSAF ...\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Length of dataframe: 2181\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on govsingapore ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on mhasingapore ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on MOEsg ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on MFAsg ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on sporeMOH ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "\n",
      "[*] Collection Successful. Total number of scrapped users: 10\n",
      "Moving files to designated folder...\n",
      "[*]--------------------------------------      COMPLETE      --------------------------------------[*]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mine Singapore accounts\n",
    "Twint_Scrapper(2500, SG_Stratum, True, True, True)\n",
    "move_verified_datasets([file + '.csv' for file in SG_Stratum], 'SG_Government')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a33e9-d926-4ad6-8f8c-bbc3c60cf690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Tweets on gunsnroses ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on Metallica ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on NASA ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on SpaceX ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on Google ...\n",
      "Length of dataframe: 2500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "\n",
      "[*] Collection Successful. Total number of scrapped users: 5\n",
      "Moving files to designated folder...\n",
      "[*]--------------------------------------      COMPLETE      --------------------------------------[*]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mine EST accounts\n",
    "Twint_Scrapper(2500, EST_Stratum, True, True, True)\n",
    "move_verified_datasets([file + '.csv' for file in EST_Stratum], 'EST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52845f2d-07a0-46ad-b8e7-3a42ff85f4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Tweets on CNN ...\n",
      "Length of dataframe: 3500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on BBCWorld ...\n",
      "Length of dataframe: 3500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on nytimes ...\n",
      "Length of dataframe: 3500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on TIME ...\n",
      "Length of dataframe: 3500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on washingtonpost ...\n",
      "Length of dataframe: 3500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on straits_times ...\n",
      "Length of dataframe: 3500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on ChannelNewsAsia ...\n",
      "Length of dataframe: 3500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on TODAYonline ...\n",
      "Length of dataframe: 3500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on WSJ ...\n",
      "Length of dataframe: 3500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on Reuters ...\n",
      "Length of dataframe: 3500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on TheEconomist ...\n",
      "Length of dataframe: 3500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on FT ...\n",
      "Length of dataframe: 3500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on business ...\n",
      "Length of dataframe: 3500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on Forbes ...\n",
      "Length of dataframe: 3500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on CNBC ...\n",
      "Length of dataframe: 3500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on MSNBC ...\n",
      "Length of dataframe: 3500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on CBSNews ...\n",
      "Length of dataframe: 3500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on ABC ...\n",
      "Length of dataframe: 3500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on cnni ...\n",
      "Length of dataframe: 3500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "Collecting Tweets on nytimesworld ...\n",
      "Length of dataframe: 3500\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "\n",
      "[*] Collection Successful. Total number of scrapped users: 20\n",
      "Moving files to designated folder...\n",
      "[*]--------------------------------------      COMPLETE      --------------------------------------[*]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mine international news accounts\n",
    "Twint_Scrapper(3500, International_News_Stratum, True, True, True) # need to mine more for international news\n",
    "move_verified_datasets([file + '.csv' for file in International_News_Stratum], 'International_News')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc23f6-8306-4ccf-964e-7bdb8f691e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    }
   ],
   "source": [
    "# need to do something to the SPF dataset because it mined too many rows\n",
    "verified_path = 'D:/SPOTTED Data Collection/Data/Verified/'\n",
    "SPF_df = pd.read_csv(verified_path + 'SG_Government/SingaporePolice.csv')\n",
    "SPF_df = SPF_df.drop(np.arange(2500, 2513)).reset_index(drop = True)\n",
    "SPF_df.to_csv(verified_path + 'SG_Government/SingaporePolice.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363fd40-599b-4994-9638-21eee735dc48",
   "metadata": {},
   "source": [
    "After downloading the required tweets, we combine the corresponding datasets into a single dataframe according to their strata. Then, we randomly sample from the 5 different dataframe according Table 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d66ef0-c4ce-433d-a145-88c49f590220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of datasets in US_Politics :\n",
      " ['POTUS.csv', 'VP.csv', 'WhiteHouse.csv', 'BarackObama.csv', 'HillaryClinton.csv'] \n",
      "\n",
      "Length of POTUS.csv dataframe is 2056\n",
      "Length of VP.csv dataframe is 2500\n",
      "Length of WhiteHouse.csv dataframe is 2283\n",
      "Length of BarackObama.csv dataframe is 2500\n",
      "Length of HillaryClinton.csv dataframe is 2500\n",
      "Length of merged dataframe is 11839, [True]\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "List of datasets in US_Military :\n",
      " ['DeptofDefense.csv', 'US_CYBERCOM.csv', 'DefenseIntel.csv', 'NSAGov.csv', 'CIA.csv', 'USArmy.csv', 'usairforce.csv', 'USNavy.csv', 'USMC.csv', 'INDOPACOM.csv'] \n",
      "\n",
      "Length of DeptofDefense.csv dataframe is 2500\n",
      "Length of US_CYBERCOM.csv dataframe is 334\n",
      "Length of DefenseIntel.csv dataframe is 2376\n",
      "Length of NSAGov.csv dataframe is 2500\n",
      "Length of CIA.csv dataframe is 2500\n",
      "Length of USArmy.csv dataframe is 2500\n",
      "Length of usairforce.csv dataframe is 2500\n",
      "Length of USNavy.csv dataframe is 2500\n",
      "Length of USMC.csv dataframe is 2500\n",
      "Length of INDOPACOM.csv dataframe is 2500\n",
      "Length of merged dataframe is 22710, [True]\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "List of datasets in SG_Government :\n",
      " ['PAPSingapore.csv', 'wpsg.csv', 'mindefsg.csv', 'SingaporePolice.csv', 'TheRSAF.csv', 'govsingapore.csv', 'mhasingapore.csv', 'MOEsg.csv', 'MFAsg.csv', 'sporeMOH.csv'] \n",
      "\n",
      "Length of PAPSingapore.csv dataframe is 2500\n",
      "Length of wpsg.csv dataframe is 2500\n",
      "Length of mindefsg.csv dataframe is 2500\n",
      "Length of SingaporePolice.csv dataframe is 2500\n",
      "Length of TheRSAF.csv dataframe is 2181\n",
      "Length of govsingapore.csv dataframe is 2500\n",
      "Length of mhasingapore.csv dataframe is 2500\n",
      "Length of MOEsg.csv dataframe is 2560\n",
      "Length of MFAsg.csv dataframe is 2500\n",
      "Length of sporeMOH.csv dataframe is 2500\n",
      "Length of merged dataframe is 24741, [True]\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "List of datasets in EST :\n",
      " ['gunsnroses.csv', 'Metallica.csv', 'NASA.csv', 'SpaceX.csv', 'Google.csv'] \n",
      "\n",
      "Length of gunsnroses.csv dataframe is 2500\n",
      "Length of Metallica.csv dataframe is 2500\n",
      "Length of NASA.csv dataframe is 2500\n",
      "Length of SpaceX.csv dataframe is 2500\n",
      "Length of Google.csv dataframe is 2500\n",
      "Length of merged dataframe is 12500, [True]\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n",
      "List of datasets in International_News :\n",
      " ['CNN.csv', 'BBCWorld.csv', 'nytimes.csv', 'TIME.csv', 'washingtonpost.csv', 'straits_times.csv', 'ChannelNewsAsia.csv', 'TODAYonline.csv', 'WSJ.csv', 'Reuters.csv', 'TheEconomist.csv', 'FT.csv', 'business.csv', 'Forbes.csv', 'CNBC.csv', 'MSNBC.csv', 'CBSNews.csv', 'ABC.csv', 'cnni.csv', 'nytimesworld.csv'] \n",
      "\n",
      "Length of CNN.csv dataframe is 3500\n",
      "Length of BBCWorld.csv dataframe is 3500\n",
      "Length of nytimes.csv dataframe is 3500\n",
      "Length of TIME.csv dataframe is 3500\n",
      "Length of washingtonpost.csv dataframe is 3500\n",
      "Length of straits_times.csv dataframe is 3500\n",
      "Length of ChannelNewsAsia.csv dataframe is 3500\n",
      "Length of TODAYonline.csv dataframe is 3500\n",
      "Length of WSJ.csv dataframe is 3500\n",
      "Length of Reuters.csv dataframe is 3500\n",
      "Length of TheEconomist.csv dataframe is 3500\n",
      "Length of FT.csv dataframe is 3500\n",
      "Length of business.csv dataframe is 3500\n",
      "Length of Forbes.csv dataframe is 3500\n",
      "Length of CNBC.csv dataframe is 3500\n",
      "Length of MSNBC.csv dataframe is 3500\n",
      "Length of CBSNews.csv dataframe is 3500\n",
      "Length of ABC.csv dataframe is 3500\n",
      "Length of cnni.csv dataframe is 3500\n",
      "Length of nytimesworld.csv dataframe is 3500\n",
      "Length of merged dataframe is 70000, [True]\n",
      "[*]--------------------------------------      SUCCESS      --------------------------------------[*]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "verified_path = 'D:/SPOTTED Data Collection/Data/Verified/'\n",
    "verified_directories = ['US_Politics', 'US_Military', 'SG_Government', 'EST', 'International_News']\n",
    "\n",
    "# get the files in each of these directories and merge all of them into one file\n",
    "df_merged_all = []\n",
    "for directory in verified_directories:\n",
    "    verified_datasets = os.listdir(verified_path + directory)\n",
    "    print('List of datasets in', directory, ':\\n', verified_datasets, '\\n')\n",
    "    \n",
    "    merged_df = dataset_fusion(verified_path + '/{}/'.format(directory), verified_datasets)\n",
    "    df_merged_all.append(merged_df)\n",
    "    \n",
    "# sample from each of the merged dataframes according to the number of rows in Table 2\n",
    "verified_df_sampled = []\n",
    "verified_df_nrows = [10000, 10000, 10000, 10000, 70000]\n",
    "for i in range(5):\n",
    "    sample_df = df_merged_all[i].sample(n = verified_df_nrows[i], random_state = i)\n",
    "    verified_df_sampled.append(sample_df)\n",
    "\n",
    "# lastly, concatenate them together\n",
    "verified_df = pd.concat(verified_df_sampled, ignore_index = True)\n",
    "    \n",
    "# randomly shuffle the dataframe before writing to csv with random state 15\n",
    "verified_df = verified_df.sample(frac = 1.0, random_state = 15)\n",
    "verified_df.to_csv(verified_path + 'Verfied_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70dc7e7-4ccb-479e-8e68-9b977c215810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c8c89e2-68c0-41dd-a908-002fcdd1d5b1",
   "metadata": {},
   "source": [
    "### Dataset Fusion\n",
    "\n",
    "Now that we have the troll and clean datasets, we are ready to fuse them together, and write the results as a csv file. Here, we will only pick the required columns that we will use later in the project and drop the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd05619-28ce-4629-a0d5-bed89c0ddfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping duplicates... Initial length of dataframe: 183920\n",
      "Removal Completed. Final length of dataframe: 168454\n"
     ]
    }
   ],
   "source": [
    "troll_all_df = pd.read_csv(troll_path + 'Troll_Dataset_Combined.csv', low_memory = False)\n",
    "\n",
    "# drop the duplicates\n",
    "troll_all_df = remove_df_duplicates(troll_all_df)\n",
    "    \n",
    "# take a random sample of 100000 rows from the merged troll dataframe\n",
    "troll_df = troll_all_df.sample(n = 100000, random_state = 8)\n",
    "\n",
    "troll_df = troll_df[['tweet_text', 'hashtags']].reset_index(drop = True)\n",
    "troll_df['target'] = np.ones(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f198fff-236f-4dbf-99f6-d4f157fa5281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping duplicates... Initial length of dataframe: 110000\n",
      "Removal Completed. Final length of dataframe: 109590\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15369</th>\n",
       "      <td>Harry Reid remembered for reshaping Obama presidency, Senate and Supreme Court by friends and foes  https://t.co/8aKYuUYGew</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18113</th>\n",
       "      <td>The Chinese army has been buying hundreds of new helicopters in just a few years' time  https://t.co/gUSw0xmJa0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71338</th>\n",
       "      <td>Cats and birds being hoarded in Radin Mas cemetery hut by self-proclaimed spirit healer  https://t.co/374ETpRUo7</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5880</th>\n",
       "      <td>#ForbesUnder30 Steve Wen, CEO of Dray Alliance, had the idea to modernize freight logistics while running a business that exported luxury goods. \"In the shipping world, they were still faxing paperwork around,\" he says. \"That made no sense to me.\"  https://t.co/eBEs1DJtcu</td>\n",
       "      <td>['forbesunder30']</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44775</th>\n",
       "      <td>Since the start of the #COVID19 pandemic, the DIA workforce has gone above &amp;amp; beyond to continue the mission. It goes without saying, DIA's workforce is simply unmatched!   On this #EmployeeAppreciationDay, join DIA Chief of Staff Johnny Sawyer in thanking the men &amp;amp; women of DIA.  https://t.co/q1v9Gir9Jl</td>\n",
       "      <td>['covid19', 'employeeappreciationday']</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                     tweet_text  \\\n",
       "15369                                                                                                                                                                                               Harry Reid remembered for reshaping Obama presidency, Senate and Supreme Court by friends and foes  https://t.co/8aKYuUYGew   \n",
       "18113                                                                                                                                                                                                           The Chinese army has been buying hundreds of new helicopters in just a few years' time  https://t.co/gUSw0xmJa0   \n",
       "71338                                                                                                                                                                                                          Cats and birds being hoarded in Radin Mas cemetery hut by self-proclaimed spirit healer  https://t.co/374ETpRUo7   \n",
       "5880                                           #ForbesUnder30 Steve Wen, CEO of Dray Alliance, had the idea to modernize freight logistics while running a business that exported luxury goods. \"In the shipping world, they were still faxing paperwork around,\" he says. \"That made no sense to me.\"  https://t.co/eBEs1DJtcu   \n",
       "44775  Since the start of the #COVID19 pandemic, the DIA workforce has gone above &amp; beyond to continue the mission. It goes without saying, DIA's workforce is simply unmatched!   On this #EmployeeAppreciationDay, join DIA Chief of Staff Johnny Sawyer in thanking the men &amp; women of DIA.  https://t.co/q1v9Gir9Jl   \n",
       "\n",
       "                                     hashtags  target  \n",
       "15369                                      []     0.0  \n",
       "18113                                      []     0.0  \n",
       "71338                                      []     0.0  \n",
       "5880                        ['forbesunder30']     0.0  \n",
       "44775  ['covid19', 'employeeappreciationday']     0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verified_all_df = pd.read_csv(verified_path + 'Verfied_Dataset.csv', low_memory = False)\n",
    "verified_all_df = verified_all_df[['tweet', 'hashtags']]\n",
    "verified_all_df = verified_all_df.rename({'tweet' : 'tweet_text'}, axis = 'columns')\n",
    "\n",
    "# drop the duplicates\n",
    "verified_all_df = remove_df_duplicates(verified_all_df)\n",
    "\n",
    "# take a random sample of 100000 rows from the merged troll dataframe\n",
    "verified_df = verified_all_df.sample(n = 100000, random_state = 67)\n",
    "\n",
    "verified_df['target'] = np.zeros(100000)\n",
    "verified_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa8bd6d-88d2-4231-a767-d9eb3c6cef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two dataframes together!\n",
    "SPOTTED_df = pd.concat([troll_df, verified_df], ignore_index = True)\n",
    "\n",
    "SPOTTED_df = SPOTTED_df.sample(frac = 1.0, random_state = 8).reset_index(drop = True)\n",
    "\n",
    "# now split the dataframe - top half is test set and the bottom half is validation set\n",
    "SPOTTED_test_df = SPOTTED_df[:150000].reset_index(drop = True)\n",
    "SPOTTED_validation_df = SPOTTED_df[-5000:].reset_index(drop = True)\n",
    "\n",
    "SPOTTED_test_df.to_csv('SPOTTED_test_dataset.csv')\n",
    "SPOTTED_validation_df.to_csv('SPOTTED_validation_dataset.csv')\n",
    "SPOTTED_df.to_csv('SPOTTED_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e9e3f0-5150-45f4-9e10-57d30a27558d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As of 5 June 2020, 12pm, we have preliminarily confirmed an additional 261 cases of COVID-19 infection in Singapore.  https://t.co/2RFMhrRkUw</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boyfriend of missing Florida woman charged with murder: \"We wish Collin would provide us the information of where Kathleen is\"   https://t.co/DBDJS5McdW</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K-pop's BTS snags top prize at American Music Awards  https://t.co/eR432aHJlm</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @CincinnatiDays: Man killed in Bond Hill after altercation  #news</td>\n",
       "      <td>[news]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jared paying attention to his video game more than me pt 2 @juliakim52 http://t.co/0AHkR3K7Vg</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cat thought lost in Kentucky tornado found 9 days later: \"I thought I heard a meow\"   https://t.co/iUGJr3kGDW</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5 Little Things You Can Do That Have Compounding Effects On Your Savings  https://t.co/3pSaMRZROB  https://t.co/Z28gZSfVGP</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @mashabletech: #Apple Reportedly Buys PrimeSense for $345 Million http://t.co/HWDdDRWoHI</td>\n",
       "      <td>['Apple']</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>We always say that early diagnosis is the best treatment for cancer. The chance of recovery is higher. Likewise, for COVID-19, if there is a way to prevent it, why not?\"    Teo Khee Huat, 78, colorectal and skin cancer survivor  Read more:  https://t.co/FR2QKY6N9P #IGotMyShotSG  https://t.co/HPqB23CKhG</td>\n",
       "      <td>['igotmyshotsg']</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@aquarius021501 Hi there. Are you getting a specific error message when you try signing into your Google account? Without revealing your email address, give us the exact wording &amp;amp; we'll try and point you in the right direction. This guide may also help:  https://t.co/2onqlsMNnL.</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                          tweet_text  \\\n",
       "0                                                                                                                                                                      As of 5 June 2020, 12pm, we have preliminarily confirmed an additional 261 cases of COVID-19 infection in Singapore.  https://t.co/2RFMhrRkUw   \n",
       "1                                                                                                                                                           Boyfriend of missing Florida woman charged with murder: \"We wish Collin would provide us the information of where Kathleen is\"   https://t.co/DBDJS5McdW   \n",
       "2                                                                                                                                                                                                                                      K-pop's BTS snags top prize at American Music Awards  https://t.co/eR432aHJlm   \n",
       "3                                                                                                                                                                                                                                               RT @CincinnatiDays: Man killed in Bond Hill after altercation  #news   \n",
       "4                                                                                                                                                                                                                      Jared paying attention to his video game more than me pt 2 @juliakim52 http://t.co/0AHkR3K7Vg   \n",
       "5                                                                                                                                                                                                      Cat thought lost in Kentucky tornado found 9 days later: \"I thought I heard a meow\"   https://t.co/iUGJr3kGDW   \n",
       "6                                                                                                                                                                                         5 Little Things You Can Do That Have Compounding Effects On Your Savings  https://t.co/3pSaMRZROB  https://t.co/Z28gZSfVGP   \n",
       "7                                                                                                                                                                                                                        RT @mashabletech: #Apple Reportedly Buys PrimeSense for $345 Million http://t.co/HWDdDRWoHI   \n",
       "8  We always say that early diagnosis is the best treatment for cancer. The chance of recovery is higher. Likewise, for COVID-19, if there is a way to prevent it, why not?\"    Teo Khee Huat, 78, colorectal and skin cancer survivor  Read more:  https://t.co/FR2QKY6N9P #IGotMyShotSG  https://t.co/HPqB23CKhG   \n",
       "9                        @aquarius021501 Hi there. Are you getting a specific error message when you try signing into your Google account? Without revealing your email address, give us the exact wording &amp; we'll try and point you in the right direction. This guide may also help:  https://t.co/2onqlsMNnL.   \n",
       "\n",
       "           hashtags  target  \n",
       "0                []     0.0  \n",
       "1                []     0.0  \n",
       "2                []     0.0  \n",
       "3            [news]     1.0  \n",
       "4                []     1.0  \n",
       "5                []     0.0  \n",
       "6                []     0.0  \n",
       "7         ['Apple']     1.0  \n",
       "8  ['igotmyshotsg']     0.0  \n",
       "9                []     0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(SPOTTED_df))\n",
    "SPOTTED_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a3dd1-5956-495b-a5de-9f4213e53b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>Centuries-old Good Shepherd ring recovered from shipwrecks off Israel  https://t.co/R97shKaKAD  https://t.co/nO5bbwwxiy</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>Here are the 100 best inventions of 2021 making the world better, smarter and a little more fun  https://t.co/fpT7v4Ayf9</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>RT @FarhanKVirk: #ArmyActForDawn Those who fulfill agenda against Pakistan should have no role in mainstream journalism https://t.co/Xsuo1K</td>\n",
       "      <td>['ArmyActForDawn']</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>@Australian_Navy long range frigate HMAS Anzac   conducts an underway replenishment with USNS Tippecanoe. #FreeAndOpenIndoPacific  https://t.co/6fmP1yBmaA</td>\n",
       "      <td>['freeandopenindopacific']</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>Although many of us might think we are done with COVID-19, its not done with us.  World Health Organization Director-General Tedros Ghebreyesus warned Monday that the Omicron variant is the latest reminder the pandemic remains an ongoing global threat  https://t.co/88ZH0B8xYq  https://t.co/ahayiAGBp6</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                              tweet_text  \\\n",
       "149995                                                                                                                                                                                           Centuries-old Good Shepherd ring recovered from shipwrecks off Israel  https://t.co/R97shKaKAD  https://t.co/nO5bbwwxiy   \n",
       "149996                                                                                                                                                                                          Here are the 100 best inventions of 2021 making the world better, smarter and a little more fun  https://t.co/fpT7v4Ayf9   \n",
       "149997                                                                                                                                                                      RT @FarhanKVirk: #ArmyActForDawn Those who fulfill agenda against Pakistan should have no role in mainstream journalism https://t.co/Xsuo1K   \n",
       "149998                                                                                                                                                        @Australian_Navy long range frigate HMAS Anzac   conducts an underway replenishment with USNS Tippecanoe. #FreeAndOpenIndoPacific  https://t.co/6fmP1yBmaA   \n",
       "149999  Although many of us might think we are done with COVID-19, its not done with us.  World Health Organization Director-General Tedros Ghebreyesus warned Monday that the Omicron variant is the latest reminder the pandemic remains an ongoing global threat  https://t.co/88ZH0B8xYq  https://t.co/ahayiAGBp6   \n",
       "\n",
       "                          hashtags  target  \n",
       "149995                          []     0.0  \n",
       "149996                          []     0.0  \n",
       "149997          ['ArmyActForDawn']     1.0  \n",
       "149998  ['freeandopenindopacific']     0.0  \n",
       "149999                          []     0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(SPOTTED_test_df))\n",
    "SPOTTED_test_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea2dbad-f472-4b78-b5bf-48dfdee7ad7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @kodiak149: .@PetersonUtah deserves more followers \\nFollow @PetersonUtah \\nSupport @PetersonUtah \\nElections matter \\n#wtpBlue https://t.co</td>\n",
       "      <td>['wtpBlue']</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@unpuNISHAble_ @Desh3hunna like you'll avi's</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @MuslimIQ: Meanwhile millions of parents find it hard to cope that 1 in 6 kids go to bed hungry every night in America, &amp;amp; the poverty th</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.@TheRock put his #RedNotice co-stars @GalGadot and @VancityReynolds to the test in our first #MuseumFaceOff!  Follow @GoogleArts and find out which one of them can tell their Michelangelo from their Van Gogh.  https://t.co/UIoTCadGzz @NetflixFilm  https://t.co/ksE0Vg8zbR</td>\n",
       "      <td>['rednotice', 'museumfaceoff']</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Parents who could work from home tried to multitask their way through, often at the cost of their productivity, sanity or both  https://t.co/K0fzLY9kVe</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @dagr8fm: New post: Chance the Rapper Finally Becomes a Jeopardy! Answer https://t.co/T5GxaANo4C</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A merry Christmas to all, and to all a good night!  https://t.co/FOcMKZM87x</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CDC shortens isolation period for asymptomatic people who test positive for Covid.  https://t.co/ugv8ibGFNi</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Congressman Elijah Cummings fought for the soul of America and will always be remembered as a giant in Congress. On the one year anniversary of his passing, may his legacy continue to shine brightly.  https://t.co/0wFn4znfOJ</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A Bloomberg Businessweek investigation shows popular ratings driving trillions into sustainable investing have little connection with a company's impact on the planet  https://t.co/GN4pdoKcm2 via @BW</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                          tweet_text  \\\n",
       "0                                                                                                                                   RT @kodiak149: .@PetersonUtah deserves more followers \\nFollow @PetersonUtah \\nSupport @PetersonUtah \\nElections matter \\n#wtpBlue https://t.co   \n",
       "1                                                                                                                                                                                                                                       @unpuNISHAble_ @Desh3hunna like you'll avi's   \n",
       "2                                                                                                                                   RT @MuslimIQ: Meanwhile millions of parents find it hard to cope that 1 in 6 kids go to bed hungry every night in America, &amp; the poverty th   \n",
       "3  .@TheRock put his #RedNotice co-stars @GalGadot and @VancityReynolds to the test in our first #MuseumFaceOff!  Follow @GoogleArts and find out which one of them can tell their Michelangelo from their Van Gogh.  https://t.co/UIoTCadGzz @NetflixFilm  https://t.co/ksE0Vg8zbR   \n",
       "4                                                                                                                            Parents who could work from home tried to multitask their way through, often at the cost of their productivity, sanity or both  https://t.co/K0fzLY9kVe   \n",
       "5                                                                                                                                                                              RT @dagr8fm: New post: Chance the Rapper Finally Becomes a Jeopardy! Answer https://t.co/T5GxaANo4C   \n",
       "6                                                                                                                                                                                                        A merry Christmas to all, and to all a good night!  https://t.co/FOcMKZM87x   \n",
       "7                                                                                                                                                                        CDC shortens isolation period for asymptomatic people who test positive for Covid.  https://t.co/ugv8ibGFNi   \n",
       "8                                                   Congressman Elijah Cummings fought for the soul of America and will always be remembered as a giant in Congress. On the one year anniversary of his passing, may his legacy continue to shine brightly.  https://t.co/0wFn4znfOJ   \n",
       "9                                                                            A Bloomberg Businessweek investigation shows popular ratings driving trillions into sustainable investing have little connection with a company's impact on the planet  https://t.co/GN4pdoKcm2 via @BW   \n",
       "\n",
       "                         hashtags  target  \n",
       "0                     ['wtpBlue']     1.0  \n",
       "1                              []     1.0  \n",
       "2                              []     1.0  \n",
       "3  ['rednotice', 'museumfaceoff']     0.0  \n",
       "4                              []     0.0  \n",
       "5                              []     1.0  \n",
       "6                              []     0.0  \n",
       "7                              []     0.0  \n",
       "8                              []     0.0  \n",
       "9                              []     0.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(SPOTTED_validation_df))\n",
    "SPOTTED_validation_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f52326-fd3e-4950-9c4a-25107ff4bc4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4144cf7-432f-4129-ac8f-87ab209e2607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "090551e611b0c68612b7ab0dc8e57f507cf5f745a51006d56d17deb45be00dce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
